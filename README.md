I experimented with two different implementation, and submitted with reduced samples to avoid longer running time.  My first imlementation is based on Vennila GAN model and the second one is beasedon CycleGAM Model.

# Overview of GAN

The algorithm implemented here is a type of Generative Adversarial Network (GAN) specifically designed for image-to-image translation. The architecture is inspired by the Vennila GAN, which utilizes a U-Net based generator and a PatchGAN discriminator. This GAN is a Conditional GAN (cGAN) used for image-to-image translation. It is designed to convert input images (photos) into output images (Monet-style paintings).

## Components of Vennila GAN

### Generator (U-Net):

The generator is responsible for creating images. It takes an input image (a photo) and generates an output image (a Monet-style painting).

The U-Net architecture is used, which consists of an encoder (downsampling path) and a decoder (upsampling path) with skip connections between layers of equal resolution in the encoder and decoder.

The encoder progressively reduces the spatial dimensions of the image while increasing the number of feature maps, capturing the context of the image.

The decoder progressively increases the spatial dimensions while reducing the number of feature maps, reconstructing the image from the encoded context.

### Discriminator (PatchGAN):

The discriminator's role is to distinguish between real images (actual Monet paintings) and fake images (generated by the generator).

Instead of classifying the entire image, a PatchGAN classifier classifies 70x70 patches of the image, making it more effective at focusing on high-frequency local information.

This discriminator outputs a matrix of predictions, where each prediction corresponds to a patch of the image being real or fake.

### Loss Functions:

`Generator Loss:` The loss functions used here are
1. Adversarial Loss: Measures RSE value between discriminators reponse for generated image and real image
2. L1 Loss: Ensures that the generated image is similar to the input photo in terms of pixel values, which helps in maintaining the structure of the input image.

`Discriminator Loss:`
1. Real Loss: Measures how well the discriminator can classify real images as real.
2. Fake Loss: Measures how well the discriminator can classify fake images as fake.

# CycleGAN

After the vanila GAN implementation, I wanted to experiment with CycleGAN, mainly based on Karas documentation.

The CycleGAN model is designed for unpaired image-to-image translation, meaning it can learn to translate images from one domain (e.g., photos) to another domain (e.g., Monet paintings) without needing paired examples.

## Components

### Generator:

Consists of convolutional layers for down-sampling, residual blocks for transformation, and transposed convolutional layers for up-sampling.

### Discriminator:

Uses convolutional layers with LeakyReLU activations to classify images as real or fake.

### Loss Functions:

`Discriminator Loss`: Measures how well the discriminator distinguishes real images from generated (fake) images.

`Generator Loss`: Measures how well the generator fools the discriminator.

`Cycle Consistency Loss`: Ensures that an image translated to the target domain and back to the original domain remains the same.

`Identity Loss`: Encourages the generator to preserve color composition between the input and output.

### Optimizers:

Adam optimizers with a learning rate of 2e-4 and beta1 of 0.5 are used for both generators and discriminators.